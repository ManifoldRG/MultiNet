{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddbf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys, os, json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "from pathlib import Path\n",
    "\n",
    "cur_dir = Path(os.path.abspath(\"\")).parent\n",
    "project_root = cur_dir.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from definitions.odinw import ODinWDefinitions\n",
    "from src.eval_utils import get_precision_per_class, get_recall_per_class, get_f1_per_class, get_macro_precision, get_macro_recall, get_macro_f1, calculate_tp_fp_fn_counts, get_micro_precision_from_counts, get_micro_recall_from_counts, get_micro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab34fc8",
   "metadata": {},
   "source": [
    "# Cross Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dec171",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\"#ea5545\",\n",
    "          \"#ede15b\",\n",
    "          \"#87bc45\",\n",
    "          \"#27aeef\",\n",
    "          \"#b33dc6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_preds_and_gt(data, sample_size):\n",
    "    \"\"\"Sample a fixed number of predictions and ground truth data with replacement for bootstrap analysis.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Dictionary containing 'preds' and 'gt_actions' arrays\n",
    "        sample_size (int): Number of samples to take\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing sampled 'preds' and 'gt_actions' arrays\n",
    "    \"\"\"\n",
    "    # Get the total number of samples\n",
    "    n_samples = len(data['preds'])\n",
    "    \n",
    "    # Generate random indices with replacement\n",
    "    indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "    \n",
    "    # Sample the data\n",
    "    sampled_data = {\n",
    "        'preds': np.array(data['preds'])[indices],\n",
    "        'gt_actions': np.array(data['gt_actions'])[indices]\n",
    "    }\n",
    "    \n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c5c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_and_gt_from_dict(model, data, dataset):\n",
    "    result = {}\n",
    "    if 'gpt' in model:\n",
    "        predictions = np.array(data[model][dataset]['preds']).flatten()\n",
    "        ground_truth = np.array(data[model][dataset]['gt_actions']).flatten()\n",
    "    elif model == 'pi0':\n",
    "        predictions = np.array(data[model][dataset]['all_preds']).flatten()\n",
    "        ground_truth = np.array(data[model][dataset]['all_gt']).flatten()\n",
    "\n",
    "    result[\"preds\"] = predictions\n",
    "    result[\"gt_actions\"] = ground_truth\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5412579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macro_micro_metrics_std_per_dataset_from_bootstrap_sample(model, data, dataset, sample_size=20_000, n_iterations=200, metric_type='macro'):\n",
    "    \"\"\"Calculate standard deviation of macro metrics using bootstrap sampling.\n",
    "    \n",
    "    Args:\n",
    "        model (str): Model name\n",
    "        data (dict): Results data\n",
    "        dataset (str): Dataset name\n",
    "        sample_size (int): Number of samples to take in each bootstrap iteration\n",
    "        n_iterations (int): Number of bootstrap iterations\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (precision_std, recall_std, f1_std)\n",
    "    \"\"\"\n",
    "    # Get the full dataset once, outside the loop\n",
    "    all_res = get_preds_and_gt_from_dict(model, data, dataset)\n",
    "    valid_classes = list(range(ODinWDefinitions.SUB_DATASET_CATEGORIES[dataset]))\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    sampled_data_precision = []\n",
    "    sampled_data_recall = []\n",
    "    sampled_data_f1 = []\n",
    "\n",
    "    # Perform bootstrap sampling\n",
    "    for i in range(n_iterations):\n",
    "        # Sample with replacement\n",
    "        sampled_data = sample_from_preds_and_gt(all_res, sample_size)\n",
    "        \n",
    "        # Calculate metrics on the sampled data\n",
    "        if metric_type == 'macro':\n",
    "            precisions = get_precision_per_class(sampled_data['preds'], sampled_data['gt_actions'], valid_classes)\n",
    "            recalls = get_recall_per_class(sampled_data['preds'], sampled_data['gt_actions'], valid_classes)\n",
    "            f1s = get_f1_per_class(precisions, recalls)\n",
    "            # Store macro metrics\n",
    "            sampled_data_precision.append(get_macro_precision(precisions))\n",
    "            sampled_data_recall.append(get_macro_recall(recalls))\n",
    "            sampled_data_f1.append(get_macro_f1(f1s))\n",
    "        elif metric_type == 'micro':\n",
    "            total_tp, total_fp, total_fn, valid_fp, invalid_fp = calculate_tp_fp_fn_counts(sampled_data['preds'], sampled_data['gt_actions'], valid_classes)\n",
    "            \n",
    "            precisions = get_micro_precision_from_counts(total_tp, total_fp)\n",
    "            recalls = get_micro_recall_from_counts(total_tp, total_fn)\n",
    "            f1s = get_micro_f1(precisions, recalls)\n",
    "\n",
    "            sampled_data_precision.append(precisions)\n",
    "            sampled_data_recall.append(recalls)\n",
    "            sampled_data_f1.append(f1s)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid metric type: {metric_type}\")\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    precision_std = np.std(sampled_data_precision)\n",
    "    recall_std = np.std(sampled_data_recall)\n",
    "    f1_std = np.std(sampled_data_f1)\n",
    "    \n",
    "    return precision_std, recall_std, f1_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b693cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_model_macro_micro_metric(results, models, metric='recall', metric_type='macro'):\n",
    "    \"\"\"Create grouped bar plots comparing model performance across datasets, arranged in 4 vertical subplots\"\"\"\n",
    "    \n",
    "    # Calculate metrics for each model\n",
    "    model_scores = {}\n",
    "    model_scores_std = {}\n",
    "    \n",
    "    if metric_type == 'macro':\n",
    "        if metric == 'precision':\n",
    "            metric_key = 'macro_precision'\n",
    "            metric_std_index = 0\n",
    "        elif metric == 'recall':\n",
    "            metric_key = 'macro_recall'\n",
    "            metric_std_index = 1\n",
    "        elif metric == 'f1':\n",
    "            metric_key = 'macro_f1'\n",
    "            metric_std_index = 2\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid metric: {metric}\")\n",
    "    elif metric_type == 'micro':\n",
    "        if metric == 'precision':\n",
    "            metric_key = 'precision'\n",
    "            metric_std_index = 0\n",
    "        elif metric == 'recall':\n",
    "            metric_key = 'recall'\n",
    "            metric_std_index = 1\n",
    "        elif metric == 'f1':\n",
    "            metric_key = 'f1'\n",
    "            metric_std_index = 2\n",
    "        elif metric == 'accuracy':\n",
    "            metric_key = 'exact_match_rate'\n",
    "            metric_std_index = 0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid metric: {metric}\")\n",
    "    \n",
    "        \n",
    "    # Collect scores for each model\n",
    "    for model in models:\n",
    "        scores = []\n",
    "        scores_std = []\n",
    "        for dataset in results[model].keys():\n",
    "            try:\n",
    "                std = get_macro_micro_metrics_std_per_dataset_from_bootstrap_sample(\n",
    "                    model, results, dataset, metric_type=metric_type)[metric_std_index]\n",
    "                scores_std.append(std)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {model}: {e}\")\n",
    "                scores_std.append(0.0)\n",
    "            try:\n",
    "                score = results[model][dataset][metric_key]\n",
    "                scores.append(score)\n",
    "            except (KeyError, TypeError):\n",
    "                scores.append(0)\n",
    "            \n",
    "            model_scores[model] = scores\n",
    "            model_scores_std[model] = scores_std\n",
    "    \n",
    "    metrics = {model: {} for model in models}\n",
    "    for model in models:\n",
    "        metrics[model][metric_key] = model_scores[model]\n",
    "        metrics[model][f'{metric_key}_std'] = model_scores_std[model]\n",
    "\n",
    "    subdatasets = list(results[models[0]].keys())\n",
    "    # Split datasets into groups of 4\n",
    "    num_datasets = len(subdatasets)\n",
    "    datasets_per_subplot = 4\n",
    "    num_subplots = (num_datasets + datasets_per_subplot - 1) // datasets_per_subplot\n",
    "\n",
    "    # Create figure with subplots and adjust spacing\n",
    "    fig = plt.figure(figsize=(15, 6*num_subplots))\n",
    "    # Add space at the top for the title and legend\n",
    "    gs = plt.GridSpec(num_subplots + 1, 1, height_ratios=[0.3] + [1]*num_subplots, hspace=0.2)\n",
    "    \n",
    "    # Create a special axes for the title and legend\n",
    "    title_ax = fig.add_subplot(gs[0])\n",
    "    title_ax.axis('off')  # Hide the axes\n",
    "    \n",
    "    # Create subplot axes\n",
    "    axs = [fig.add_subplot(gs[i+1]) for i in range(num_subplots)]\n",
    "\n",
    "    # Plot settings\n",
    "    bar_width = 0.15\n",
    "    opacity = 0.8\n",
    "\n",
    "    # Create plots for each group of datasets\n",
    "    for subplot_idx in range(num_subplots):\n",
    "        ax = axs[subplot_idx]\n",
    "        \n",
    "        # Get the datasets for this subplot\n",
    "        start_idx = subplot_idx * datasets_per_subplot\n",
    "        end_idx = min(start_idx + datasets_per_subplot, num_datasets)\n",
    "        current_datasets = subdatasets[start_idx:end_idx]\n",
    "        current_datasets = [ds[:15] + '...' if len(ds) > 15 else ds for ds in current_datasets]\n",
    "        # Calculate x positions\n",
    "        x = np.arange(len(current_datasets))\n",
    "        \n",
    "        # Calculate maximum y value including error bars for this subplot\n",
    "        max_y = 0\n",
    "        for model in models:\n",
    "            current_scores = model_scores[model][start_idx:end_idx]\n",
    "            current_stds = model_scores_std[model][start_idx:end_idx]\n",
    "            try:\n",
    "                max_with_error = max([score + std for score, std in zip(current_scores, current_stds) if score > 0])\n",
    "            except:\n",
    "                max_with_error = 0\n",
    "            max_y = max(max_y, max_with_error)\n",
    "        \n",
    "        # Add padding to y-axis limit (30% extra space)\n",
    "        y_limit = max_y * 1.3\n",
    "        \n",
    "        # Plot bars for each model\n",
    "        for i, model in enumerate(models):\n",
    "            # Get scores for current datasets\n",
    "            current_scores = model_scores[model][start_idx:end_idx]\n",
    "            current_stds = model_scores_std[model][start_idx:end_idx]\n",
    "            \n",
    "            # Plot bars with error bars\n",
    "            bars = ax.bar(x + i*bar_width, current_scores, bar_width,\n",
    "                         alpha=opacity, color=COLORS[i], label=model,\n",
    "                         yerr=current_stds, capsize=5, error_kw={'elinewidth': 2})\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for idx, (value, std) in enumerate(zip(current_scores, current_stds)):\n",
    "                if value > 0:  # Only show non-zero values\n",
    "                    # Calculate label position\n",
    "                    label_height = value + std\n",
    "                    if label_height > y_limit * 0.85:  # If label would be too close to top\n",
    "                        label_height = value - std  # Place label below error bar\n",
    "                        va = 'top'\n",
    "                    else:\n",
    "                        va = 'bottom'\n",
    "                    \n",
    "                    ax.text(x[idx] + i*bar_width, label_height,\n",
    "                           f'{value:.2f}', ha='center', va=va,\n",
    "                           rotation=45, fontsize=16)\n",
    "        \n",
    "        # Customize subplot\n",
    "        if metric != 'accuracy':\n",
    "            ax.set_ylabel(f'{metric_type.capitalize()} {metric.capitalize()}', fontsize=20)\n",
    "        else:\n",
    "            ax.set_ylabel(f'Accuracy', fontsize=20)\n",
    "        ax.set_xticks(x + bar_width * (len(models)-1)/2)\n",
    "\n",
    "        ax.set_xticklabels(current_datasets, ha='right', fontsize=20)\n",
    "        ax.grid(True, axis='y', alpha=0.3)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "        \n",
    "        # Set y-axis limits\n",
    "        ax.set_ylim(0, y_limit)\n",
    "\n",
    "    # Add overall title\n",
    "    if metric != 'accuracy':\n",
    "        title = f'Model {metric_type.capitalize()} {metric.capitalize()} Comparison for ODinW'\n",
    "    else:\n",
    "        title = f'Model Accuracy Comparison for ODinW'\n",
    "\n",
    "    title_ax.text(0.5, 0.7, title, fontsize=24, ha='center', va='bottom')\n",
    "\n",
    "    # Create a single legend for all subplots\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    title_ax.legend(handles, labels, \n",
    "                   loc='center',\n",
    "                   bbox_to_anchor=(0.5, 0.2),\n",
    "                   ncol=len(models) + 1,  # +1 for random line only\n",
    "                   fontsize=20)\n",
    "\n",
    "    # Save the plot with high DPI\n",
    "    if metric != 'accuracy':\n",
    "        filename = f'model_comparison_{metric_type}_{metric_key}_with_invalids_odinw.png'\n",
    "    else:\n",
    "        filename = f'model_comparison_accuracy_odinw.png'\n",
    "    output_path = os.path.abspath(os.path.join(\"plots\", filename))\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25056619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gpt5 results\n",
    "models = ['gpt5', 'pi0']\n",
    "all_results = {'gpt5': {}, 'pi0': {}}\n",
    "for dataset in ODinWDefinitions.SUB_DATASET_CATEGORIES.keys():\n",
    "    gpt5_results_dir = f\"{project_root}/src/v1/results/genesis/gpt_5/low_reasoning/odinw/\"\n",
    "    results_file = f\"{dataset}_results.json\"\n",
    "    with open(Path(gpt5_results_dir) / results_file, 'r') as f:\n",
    "        gpt5_results = json.load(f)[dataset]\n",
    "\n",
    "    # Load pi0 results\n",
    "    pi0_results_dir = f\"{project_root}/src/v1/results/pi0/odinw\"\n",
    "    results_file = f\"pi0_hf_odinw_{dataset}_inference_results.json\"\n",
    "    with open(Path(pi0_results_dir) / results_file, 'r') as f:\n",
    "        pi0_results = json.load(f)\n",
    "\n",
    "    all_results['gpt5'][dataset] = gpt5_results\n",
    "    all_results['pi0'][dataset] = pi0_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b54d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_model_macro_micro_metric(all_results, models, metric='recall', metric_type='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "237e54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_model_macro_micro_metric(all_results, models, metric='accuracy', metric_type='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163ee70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multinet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
